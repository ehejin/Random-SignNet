
Epoch: 001, Train Loss: 0.9058, Val: -0.8366, Test: -0.8643, Seconds: 163.3772, Memory Peak: 30710 MB allocated, 45048 MB reserved.
Epoch: 002, Train Loss: 0.5241, Val: -0.7127, Test: -0.7503, Seconds: 178.2997, Memory Peak: 30710 MB allocated, 45314 MB reserved.
Epoch: 003, Train Loss: 0.4863, Val: -0.4585, Test: -0.4947, Seconds: 179.3936, Memory Peak: 30710 MB allocated, 45314 MB reserved.
Epoch: 004, Train Loss: 0.4771, Val: -0.5410, Test: -0.4947, Seconds: 175.4470, Memory Peak: 30710 MB allocated, 45314 MB reserved.
Epoch: 005, Train Loss: 0.4533, Val: -0.9500, Test: -0.4947, Seconds: 173.3268, Memory Peak: 30710 MB allocated, 45314 MB reserved.
Epoch: 006, Train Loss: 0.4456, Val: -0.9433, Test: -0.4947, Seconds: 175.0061, Memory Peak: 30710 MB allocated, 45314 MB reserved.
Epoch: 007, Train Loss: 0.4386, Val: -0.7003, Test: -0.4947, Seconds: 175.2550, Memory Peak: 30710 MB allocated, 45314 MB reserved.
Epoch: 008, Train Loss: 0.4345, Val: -0.8108, Test: -0.4947, Seconds: 174.0067, Memory Peak: 30710 MB allocated, 45314 MB reserved.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lfs/hyperturing1/0/echoi1/Random-SignNet/GINESignNetPyG/train/zinc.py", line 110, in <module>
    cfg = update_cfg(cfg)
  File "/lfs/hyperturing1/0/echoi1/Random-SignNet/GINESignNetPyG/core/train.py", line 47, in run
    train_loss = train(train_loader, model, optimizer, device=cfg.device, num_samples=cfg.train.num_samples)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lfs/hyperturing1/0/echoi1/Random-SignNet/GINESignNetPyG/train/zinc.py", line 75, in train
    with torch.autograd.set_detect_anomaly(True):
        ^^^^^^^^^^^^^^^
  File "/lfs/hyperturing1/0/echoi1/env/micromamba/envs/graphenv/lib/python3.11/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/lfs/hyperturing1/0/echoi1/env/micromamba/envs/graphenv/lib/python3.11/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/lfs/hyperturing1/0/echoi1/env/micromamba/envs/graphenv/lib/python3.11/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt