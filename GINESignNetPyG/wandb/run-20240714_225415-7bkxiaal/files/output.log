
Epoch: 001, Train Loss: 0.9382, Val: -1.3475, Test: -1.3426, Seconds: 239.8507, Memory Peak: 41448 MB allocated, 45276 MB reserved.
Epoch: 002, Train Loss: 0.5407, Val: -3.2333, Test: -1.3426, Seconds: 243.8379, Memory Peak: 41544 MB allocated, 45276 MB reserved.
Epoch: 003, Train Loss: 0.4823, Val: -1.0064, Test: -1.0522, Seconds: 249.6779, Memory Peak: 41544 MB allocated, 45276 MB reserved.
Epoch: 004, Train Loss: 0.4759, Val: -0.8209, Test: -0.8664, Seconds: 249.5997, Memory Peak: 42232 MB allocated, 45292 MB reserved.
Epoch: 005, Train Loss: 0.4597, Val: -1.2569, Test: -0.8664, Seconds: 241.5706, Memory Peak: 42232 MB allocated, 45292 MB reserved.
Epoch: 006, Train Loss: 0.4375, Val: -0.4587, Test: -0.5057, Seconds: 251.3725, Memory Peak: 42232 MB allocated, 45292 MB reserved.
Epoch: 007, Train Loss: 0.4383, Val: -0.8533, Test: -0.5057, Seconds: 246.6941, Memory Peak: 42232 MB allocated, 45316 MB reserved.
Epoch: 008, Train Loss: 0.4325, Val: -1.0466, Test: -0.5057, Seconds: 247.4078, Memory Peak: 42232 MB allocated, 45316 MB reserved.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lfs/hyperturing1/0/echoi1/Random-SignNet/GINESignNetPyG/train/zinc.py", line 110, in <module>
    cfg = update_cfg(cfg)
  File "/lfs/hyperturing1/0/echoi1/Random-SignNet/GINESignNetPyG/core/train.py", line 47, in run
    train_loss = train(train_loader, model, optimizer, device=cfg.device, num_samples=cfg.train.num_samples)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lfs/hyperturing1/0/echoi1/Random-SignNet/GINESignNetPyG/train/zinc.py", line 75, in train
    with torch.autograd.set_detect_anomaly(True):
        ^^^^^^^^^^^^^^^
  File "/lfs/hyperturing1/0/echoi1/env/micromamba/envs/graphenv/lib/python3.11/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/lfs/hyperturing1/0/echoi1/env/micromamba/envs/graphenv/lib/python3.11/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/lfs/hyperturing1/0/echoi1/env/micromamba/envs/graphenv/lib/python3.11/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt